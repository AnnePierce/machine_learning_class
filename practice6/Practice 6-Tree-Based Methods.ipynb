{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'graphviz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-3a816c2704b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgraphviz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'graphviz'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import graphviz\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 Fitting Classification Trees\n",
    "\n",
    "The ${\\tt sklearn}$ library has a lot of useful tools for constructing classification and regression trees:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import confusion_matrix, mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "${\\tt Carseats}$ data set: A data frame with 400 observations on the following 11 variables.\n",
    "\n",
    "`Sales`: Unit sales (in thousands) at each location\n",
    "\n",
    "`CompPrice`: Price charged by competitor at each location\n",
    "\n",
    "`Income`: Community income level (in thousands of dollars)\n",
    "\n",
    "`Advertising`: Local advertising budget for company at each location (in thousands of dollars)\n",
    "\n",
    "`Population`: Population size in region (in thousands)\n",
    "\n",
    "`Price`: Price company charges for car seats at each site\n",
    "\n",
    "`ShelveLoc`: A factor with levels Bad, Good and Medium indicating the quality of the shelving location for the car seats at each site\n",
    "\n",
    "`Age`: Average age of the local population\n",
    "\n",
    "`Education`: Education level at each location\n",
    "\n",
    "`Urban`: A factor with levels No and Yes to indicate whether the store is in an urban or rural location\n",
    "\n",
    "`US`: A factor with levels No and Yes to indicate whether the store is in the US or not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by using **classification trees** to analyze the ${\\tt Carseats}$ data set. In these\n",
    "data, ${\\tt Sales}$ is a continuous variable, and so we begin by converting it to a\n",
    "binary variable. We use the ${\\tt ifelse()}$ function to create a variable, called\n",
    "${\\tt High}$, which takes on a value of ${\\tt Yes}$ if the ${\\tt Sales}$ variable exceeds 8, and\n",
    "takes on a value of ${\\tt No}$ otherwise. We'll append this onto our dataFrame using the ${\\tt .map()}$ function, and then do a little data cleaning to tidy things up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unauthorized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Unauthorized]\n",
       "Index: []"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3 = pd.read_csv('Carseats.csv')\n",
    "df3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to properly evaluate the performance of a classification tree on\n",
    "the data, we must estimate the test error rather than simply computing\n",
    "the training error. We first split the observations into a training set and a test\n",
    "set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now use the ${\\tt DecisionTreeClassifier()}$ function to fit a classification tree in order to predict\n",
    "${\\tt High}$ using all variables but ${\\tt Sales}$ (that would be a little silly...).  http://scikit-learn.org/stable/modules/tree.html\n",
    "\n",
    "We can limit the depth of a tree using the ${\\tt max\\_depth}$ parameter: Set it to 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the training accuracy is 95.5%.\n",
    "\n",
    "One of the most attractive properties of trees is that they can be\n",
    "graphically displayed. Unfortunately, this is a bit of a roundabout process in ${\\tt sklearn}$. We use the ${\\tt export\\_graphviz()}$ function to export the tree structure to a temporary ${\\tt .dot}$ file,\n",
    "and the ${\\tt graphviz.Source()}$ function to display the image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important indicator of ${\\tt High}$ sales appears to be ${\\tt Price}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's evaluate the tree's performance on\n",
    "the test data. The ${\\tt predict()}$ function can be used for this purpose. We can then build a confusion matrix, which shows that we are making correct predictions for\n",
    "around 74.5% of the test data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 Fitting Regression Trees\n",
    "\n",
    "Now let's try fitting a **regression tree** to the ${\\tt Boston}$ data set. First, we create a\n",
    "training set, and fit the tree to the training data using ${\\tt medv}$ (median home value) as our response:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "${\\tt Boston}$ data set: A data frame with 506 rows and 13 variables.\n",
    "\n",
    "`crim`: per capita crime rate by town.\n",
    "\n",
    "`zn`: proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "\n",
    "`indus`: proportion of non-retail business acres per town.\n",
    "\n",
    "`chas`: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise). \n",
    "\n",
    "`nox`: nitrogen oxides concentration (parts per 10 million).\n",
    "\n",
    "`rm`: average number of rooms per dwelling.\n",
    "\n",
    "`age`: proportion of owner-occupied units built prior to 1940.\n",
    "\n",
    "`dis`: weighted mean of distances to five Boston employment centres.\n",
    "\n",
    "`rad`: index of accessibility to radial highways.\n",
    "\n",
    "`tax`: full-value property-tax rate per \\$10,000.\n",
    "\n",
    "`ptratio`: pupil-teacher ratio by town.\n",
    "\n",
    "`lstat`: lower status of the population (percent).\n",
    "\n",
    "`medv`: median value of owner-occupied homes in \\$1000s.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choosing max depth 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable ${\\tt lstat}$ measures the percentage of individuals with lower\n",
    "socioeconomic status. The tree indicates that lower values of ${\\tt lstat}$ correspond\n",
    "to more expensive houses. The tree predicts a median house price\n",
    "of $\\$ 45766 $ for larger homes ( ${\\tt rm}$ >=7.435) in suburbs in which residents have high socioeconomic\n",
    "status ( ${\\tt lstat}$ <7.81).\n",
    "\n",
    "Now let's see how it does on the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test set MSE associated with the regression tree is\n",
    "28.8. The square root of the MSE (AKA. the average distance between the predicted value and the true value) https://towardsdatascience.com/what-does-rmse-really-mean-806b65f2e48e, https://tiaplagata.medium.com/interpreting-the-root-mean-squared-error-of-a-linear-regression-model-5166e6b10db8) is therefore around 5.37, indicating\n",
    "that this model leads to test predictions that are within around \\$5,370 of\n",
    "the true median home value for the suburb."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 Bagging and Random Forests\n",
    "\n",
    "Let's see if we can improve on this result using **bagging** and **random forests**. Recall that **bagging** is simply a special case of\n",
    "a **random forest** with $m = p$. Therefore, the ${\\tt RandomForestRegressor()}$ function can\n",
    "be used to perform both random forests and bagging. Let's start with bagging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bagging: using all features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The argument ${\\tt max\\_features=13}$ indicates that all 13 predictors should be considered\n",
    "for each split of the tree -- in other words, that bagging should be done. How\n",
    "well does this bagged model perform on the test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test set MSE associated with the bagged regression tree is significantly lower than our single tree!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can grow a random forest in exactly the same way, except that\n",
    "we'll use a smaller value of the ${\\tt max\\_features}$ argument. Here we'll\n",
    "use ${\\tt max\\_features = 6}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forests: using 6 features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test set MSE is even lower; this indicates that random forests yielded an\n",
    "improvement over bagging in this case.\n",
    "\n",
    "Using the ${\\tt feature\\_importances\\_}$ attribute of the ${\\tt RandomForestRegressor}$, we can view the importance of each\n",
    "variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results indicate that across all of the trees considered in the random\n",
    "forest, the wealth level of the community (${\\tt lstat}$) and the house size (${\\tt rm}$)\n",
    "are by far the two most important variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forests: using 3 features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4 Boosting\n",
    "\n",
    "Now we'll use the ${\\tt GradientBoostingRegressor}$ package to fit **boosted\n",
    "regression trees** to the ${\\tt Boston}$ data set. The\n",
    "argument ${ n\\_estimators=500}$ indicates that we want 500 trees, and the option\n",
    "${max\\_depth=3}$ limits the depth of each tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out the feature importances again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that ${\\tt lstat}$ and ${\\tt rm}$ are again the most important variables by far. Now let's use the boosted model to predict ${\\tt medv}$ on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test MSE obtained is similar to the test MSE for random forests\n",
    "and superior to that for bagging. If we want to, we can perform boosting\n",
    "with a different value of the shrinkage parameter $\\lambda$. Here we take $\\lambda = 0.2$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, using $\\lambda = 0.2$ leads to a slightly lower test MSE than $\\lambda = 0.01$."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
